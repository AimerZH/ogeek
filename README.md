# OGeek算法挑战赛

### 文件说明

+ lgb1.py, fea1.py: 初赛个人lgb的单模和特征工程
+ lgb2.py, fea2.py: 复赛个人lgb的单模和特征工程
+ nlp.py: 个人模型中与文本处理相关的函数封装
+ lgb_offline.py: 复赛最佳lgb的线下模型代码
+ lgb_online.py: 复赛最佳lgb的线上模型代码
+ 其余队友代码均不公开

### 赛题说明

本次赛题为实时搜索场景。根据用户输入的搜索词，系统匹配的预测词列表及文章的标题和标签类型等信息，进行点击率预估。评价指标为F1。

详细信息：[OGeek算法挑战赛](https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.2a6471eebXF6aS&raceId=231688)

最终排名：初赛17， 复赛26

### 赛题思路

+ 该赛题的数据预测上，既要考虑已有历史搜索记录的数据预测，又要考虑新数据的冷启动问题。已有数据可以参考历史点击率等统计特征预测，新数据则更多需要依赖文本相似度等信息匹配特征。可以将赛题理解为一半的CTR问题跟一半的NLP问题。加上官方限制了最多使用2个模型，因此较合理的方案为1个偏向CTR预测的lgb模型+1个偏向文本匹配的NN模型。
+ 出题方为了检验模型在新旧数据上的预测能力，在数据集的构建上进行了人工干预的抽样处理，这使得数据集在训练集跟测试集的分布出现了明显的不一致情况。因此在模型训练中，不能按以往那样按照数据集分布一致的假设去设计和验证模型，必须考虑模型在不同分布的数据集上的有效性。
+ 对于数据分布不一致的问题，除了增强模型在不同分布数据上的鲁棒性外，还有一种思路就是去尝试拟合线上数据集的分布，从而使模型在设计时可以采用有分布要求的方案。
+ 赛题采用F1评价指标，在模型评价函数上可以选用logloss、auc，或者直接自定义F1函数。同时如何选取模型的阈值也是一个需要思考的点

### 分布处理方案

比赛期间采用过多种分布方案，其中主要有以下几种：

1. 统计特征采用五折交叉提取，既避免特征穿越，又能产生带有新数据的样本分布。只是采样出的分布比较粗糙，与验证集和测试集依旧有较明显差距
2. 通过对测试集的样本特征分析，从训练集中模拟出一份新数据的特征样本，加入到原数据集中构成新旧数据混合的训练数据集。具体方案见github博客
3. 由于复赛验证集与测试集同分布，因此在训练线上模型时，加大验证集数据的样本权重，使得整个训练数据更偏向验证集的分布。

### 特征工程

+ 全局特征：包括预测词个数，预测概率的最大值等统计特征
+ 统计特征：包括各维度及交叉维度的出现次数、点击次数、点击率，两维度之间的种类数统计、种类出现样本比例统计
+ 文本特征：包括文本长度，长度比和长度差，prefix、title、query文本之间的编辑距离、jaccard距离、词向量余弦相似度、词移距离等相似度计算。query各预测值与其他字段相似度的最大最小均值方差等统计特征
+ 词向量特征：从数据集训练出的50维词向量，包括prefix、title、最大概率的query，共150维的词向量维度直接作为特征丢进去

### 模型构建

【lgb模型】
+ 采用logloss损失函数。线下模型用训练集训练，验证集验证，最后在验证集上遍历搜索出最佳阈值并得到线下F1评分。
+ 线上模型采用训练集+验证集训练，迭代次数由线下模型最佳迭代确定，阈值用线下模型的最佳阈值
+ 评价指标尝试过logloss、auc、F1，最终选定了比较稳定的logloss

【nn模型】
+ 训练特征采用预训练词向量，以及lgb重要性较高的模型特征
+ 采用logloss损失函数，训练集和验证集划分方式与lgb相同。
+ 根据线下验证集表现人工确定模型迭代次数
+ 这部分由队友负责，网络细节暂且未知

### 模型融合

两模型先输出概率值，根据验证集F1效果确定两模型概率值的加权比例，再遍历查找出最佳阈值

### 后处理

基于同prefix不同title_tag的相邻数据为同一次搜索事件的假设，对预测数据集进行搜索事件的分组，每一组中若有多个样本被预测为1，则只保留最高概率样本预测为1，其余样本预测为0。

该操作在复赛验证集上，共修改了13个模型预测结果，修改正确率为100%。线上效果未知。
